---
title: "Analysis Lasso-Re (Compare Baseline Model)"
date: "`r Sys.Date()`"
author: "Cher Yang"
output:
  html_document:
    code_folding: hide
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(dplyr)
library(magrittr)
library(ggplot2)
library(ggthemes)
library(ppcor)
library(reshape2)
library(groupdata2)
library(knitr)       # kable()

#library(gglasso)
library(glmnet)
library(ggsci)
library(viridis)
library(ggExtra)
library(kableExtra)
library(xtable)
library(ggrepel)
library(scales)
library(car)
library(pROC)
library(patchwork)      # Multi-plot alignment
library(brainconn)
library(igraph)
library(network)
library(brainconn)
library(brainGraph)
library(RColorBrewer)
library(networkD3)
library(ggpubr)
library(DescTools)
library(plotly)
#library(data.table)
library(rsample)   
library(purrr)
library(dplyr)
library(ggplot2)
library(scales)
library(mlbench)
library(kernlab)
library(sessioninfo)
rm(list=ls())
```

**Analysis parameters:**

*rsfMRI data Parameters:*

```{r}
sample_method <-  "none" # methods are: up, down, random, none
```

-   rsfMRI: R1S1 mr_pcorr.txt file

-   Z norm before load: False

-   sampling method:  `r sample_method`

# Load Data

```{r}
  CONN_FILE_PATH = "../data/connectivity_matrix/REST1"
  SES_FILE_PATH = "ses-01/mr_pcorr.txt"
  POWER_FILE_PATH = "../data/power_2011.csv"
  
  power2011 <- read_csv(POWER_FILE_PATH, 
                      col_types = cols(ROI=col_double(),
                                       X = col_double(),
                                       Y = col_double(),
                                       Z = col_double(),
                                       Network = col_double(),
                                       Color = col_character(),
                                       NetworkName = col_character())) %>%
  dplyr::select(ROI, X, Y, Z, Network, Color, NetworkName)


  # color setup
  #power2011 %>% mutate(Color = factor(Color)) %>%
  power2011$Color <- recode_factor(power2011$Color, White="#8DD3C7", 
                                  Cyan="#E0FFFF", 
                                  Orange="#FF8C00", 
                                  Purple="#9370DB", 
                                  Pink="#FFB6C1", 
                                  Red="#FF6347", 
                                  Gray="#808080", 
                                  Teal="#008080", 
                                  Blue="#87CEFA", 
                                  Yellow="#FFFF66", 
                                  Black="#000000", 
                                  Brown="#8B4513", 
                                  `Pale blue`="#4682B4", 
                                  Green="#3CB371") 
  power2011$Color <- as.character(power2011$Color)
```

```{r}
# rest data subjects
rest_subjects = as.data.frame(x  = Sys.glob(paths = "../data/connectivity_matrix/REST1/*")) 
colnames(rest_subjects) <- c('subject_id')  

# clean subject id
rest_subjects <- rest_subjects %>%
  mutate(subject_id = purrr::map_chr(subject_id, ~strsplit(., "/")[[1]][5])) %>%
  mutate(HCPID = paste(str_replace(subject_id, pattern = "sub-", ""), sep = "_", "fnca"))
```

```{r}

dvs <- read_csv("../data/LL_model2.csv", #read_csv("../data/actr_maxLL.csv", 
                col_types = cols(
                  .default = col_double(),
                  HCPID = col_character(),
                  best.model = col_character(), 
                  diff.LL = col_double()
                )) %>% 
  # find common subject ID that both rest and task data are available
  inner_join(rest_subjects, by = "HCPID") %>%
  # exclude bad subjects that are hard to distinguish by two models
  #anti_join(bad_subj, by = "HCPID") %>%
  mutate(BestModel = best.model, LL.diff = diff.LL) %>%
  dplyr::select(HCPID, BestModel, LL.diff) %>%
  arrange(HCPID) %>%
  mutate(Y = as.numeric(as.factor(BestModel)) -1,  # model1 = 0, model2 = 1
         HCPID = factor(HCPID), 
         BestModel=factor(BestModel))


dvs %>% group_by(BestModel) %>% count()
```

Compare to different sampling methods: shuffle Y, up-sampling Y, and do
nothing on Y

```{r}
set.seed(0) # For reproducibility


if (sample_method == "random") {
  dvs.sampling <- dvs
  dvs.sampling$BestModel <-  sample(dvs$BestModel)
} else if (sample_method == "up") {
  dvs.sampling <- groupdata2::upsample(
  dvs,
  cat_col = "BestModel",
  id_col = "HCPID",
  id_method = "n_ids", 
  mark_new_rows = T)
} else if (sample_method == "up_random") {
  dvs.sampling <- groupdata2::upsample(
  dvs,
  cat_col = "BestModel",
  id_col = "HCPID",
  id_method = "n_ids", 
  mark_new_rows = T)
  dvs.sampling$BestModel <- sample(dvs.sampling$BestModel)

} else if (sample_method == "down") {
  dvs.sampling <- groupdata2::downsample(
  dvs,
  cat_col = "BestModel",
  id_col = "HCPID",
  id_method = "n_ids")
  
} else if (sample_method == "none") {
  dvs.sampling <- dvs
  
} else {
  print("Invalid sample_method. Please choose from: random, up, up_random, none")
}


dvs.sampling %>% group_by(BestModel) %>% count()


```

```{r}
common_subjects <- dvs.sampling %>% left_join(rest_subjects)
common_subject_ids <- common_subjects$subject_id
```

Find subject ID that both rest and task data are available. We have
common subjects: N = `r length(common_subject_ids)`

```{r}
  # Start creating $X$
  NOFLY <- c()
  SUBJS <- c()
  cols <- outer(power2011$ROI, power2011$ROI, function(x, y) {paste(x, y, sep="-")})
  cols %<>% as.vector
  
  connection <- function(x, y) {
    paste(min(x, y), max(x, y), sep="-")
  }
  
  vconnection <- Vectorize(connection)
  
  Mode <- function(x, na.rm=F) {
    if (na.rm) {
      x = x[!is.na(x)]
    }
    ux <- unique(x)
    return(ux[which.max(tabulate(match(x, ux)))])
  }
  
  reduced_power2011 <- power2011 %>% 
    dplyr::select(Network, NetworkName) %>%
    group_by(Network) %>%
    summarize(Network = mean(Network), NetworkName = Mode(NetworkName))
  
  connection_name <- function(x, y) {
    first <- min(x, y)
    second <- max(x, y)
    paste(reduced_power2011 %>% filter(Network == first) %>% dplyr::select(NetworkName) ,
          reduced_power2011 %>% filter(Network == second) %>% dplyr::select(NetworkName),
          sep="-")
    
  }
  
  vconnection_name <- Vectorize(connection_name)
  
  connection_name2 <- function(x, y) {
    first <- min(x, y)
    second <- max(x, y)
    paste(reduced_power2011$NetworkName[reduced_power2011$Network == first],
          reduced_power2011$NetworkName[reduced_power2011$Network == second],
          sep="-")
    
  }
  
  vconnection_name2 <- Vectorize(connection_name2)
  
  
  nets <- outer(power2011$Network, power2011$Network, vconnection)
  nets %<>% as.vector
  netnames <- outer(power2011$Network, power2011$Network, vconnection_name2)
  netnames %<>% as.vector
  
  
  n <- length(common_subject_ids) #length(grep("sub-*", dir(CONN_FILE_PATH)))
  C <- matrix(data = rep(0, length(cols)*n), nrow =  n)
  
  j <- 1
  
  R <- NULL
  PR <- NULL
  
  #for (sub in dir(CONN_FILE_PATH)[grep("sub-*", dir(CONN_FILE_PATH))]) {
  for (sub in common_subject_ids) {
    #SUBJS %<>% c(strsplit(sub, "-")[[1]][2])
    M <- paste(CONN_FILE_PATH, 
               sub, 
               SES_FILE_PATH, sep="/") %>%
      read_csv(skip = 1,
               col_names = F,
               col_types = cols(
                 .default = col_double(),
                 X1 = col_character()
               )) %>%
      as.matrix() 
    v <- as_vector(M[,2:265])  # v spreads M column-wise. M is symmetrical, so it should not matter, but better not risk it
    C[j,] <- v
    if (length(v[is.na(v)]) > 0) {
      print(paste("NA detected in sub", sub))
      NOFLY %<>% c(sub)  # Addes sub to NOFLY list
    }
    
    j <- j + 1
  }
  C <- apply(C, 2, FUN=as.numeric)
```

## Define the Networks

```{r}
NOI <- c(
  "Uncertain",
  "Sensory/somatomotor Hand",
  "Sensory/somatomotor Mouth",
  "Cingulo-opercular Task Control",
  "Auditory",
  "Default mode",
  "Memory retrieval?",
  "Ventral attention",
  "Visual",
  "Fronto-parietal Task Control",
  "Salience",
  "Subcortical",
  "Cerebellar",
  "Dorsal attention"
)

COI <- outer(NOI, 
             NOI, 
             function(x, y) {paste(x, y, sep="-")}) %>% as.vector()
```

The first censor vector simply removes the redundant columns (since the
connectivity from *A* to *B* is the same as the connectivity of *B* to
*A*) and the self-correlations:

```{r}
censor <- outer(power2011$ROI, 
                power2011$ROI, 
                function(x, y) {x < y}) %>% as.vector()
```

The second censor vector removes unlikely functional connections: Those
with a partial correlation value $|r| < 0.05|$.

```{r}
censor2 <- colMeans(C) %>% abs() > 0.05
```

Now, we combine the censor vectors in a tibble that contains all of the
relevant information about each column in *C*.

```{r}
order <- tibble(index = 1:length(nets), 
                network = nets, 
                network_names = netnames,
                connection = cols, 
                censor=censor,
                censor2 = censor2)
order %<>% arrange(network)
```

And we remove all entries for each a censor vector is `FALSE` (we also
create a grouping factor *G*, in case in the future we want to use
*Group* Lasso).

```{r}
I <- order %>%
  filter(censor == TRUE) %>%
  filter(censor2 == TRUE) %>%
  filter(network_names %in% COI) %>%
  dplyr::select(index) 

G <- order %>%
  filter(censor == TRUE) %>%
  filter(network_names %in% COI) %>%
  dplyr::select(network) 
# G is the real grouping factor for Lasso!
```

As a last step, we create the "real" regressor matrix $X$, which is the
proper subset of $C$ after removing all of the censored columns. Also,
we need to load the dependent variable. In this case, it is a binary
variable that determines which strategy model best fits the behavioral
data of an individual, whether it is the "memory" strategy ($Y = 1$) or
the "procedural" strategy ($Y = 2$).

```{r}
X <- C[,as_vector(I)]
```

Now we select only the rows of $X$ and the values of $Y$ for which we
have both rsfMRI and model data.

The dimension of X is: `r dim(X)`

Finally, we transform the dependent variable $Y$ into a binary numeric
variable with values $(0, 1)$, so that we can use logistic regression.

```{r}
Y <- dvs.sampling$Y
```

> Quality and Characteristics of $X$ and $Y$

Let's do some visualization and analysis of our indepedenent and
dependet variables, just to ensure there are no obvious problems.

> Collinearity of Connectivity Regressors $X$

The regressors $X$ is certainly multi-collinear; that is a consequence
of having a large number of predictors $p > n$, which, in turn, is one
of the reasons why we are using Lasso. Too much collinearity, however,
could be really bad and push Lasso towards selecting non-optimal
regressors. To gather a sense of how much collinearity we have, we can
plot the distribution of correlations among regressors:

```{r}
corX <- cor(X)
distCor <- as_vector(corX[lower.tri(corX, diag = F)])
distTibble <- as_tibble(data.frame(R=distCor))

ggplot(distTibble, aes(x=R)) +
  geom_histogram(col="white", alpha=0.5, binwidth = 0.05) +
  theme_pander() +
  ylab("Number of Correlations") +
  xlab("Correlation Value") +
  ggtitle("Distribution of Correlation Values Between Regressors")
```

All in all, the collinearity is not that bad---all regressors are
correlated at $|r| < 0.25$, and most of them are correlated at
$|r| < 0.1$, with a peak at $r = 0$.

## Distribution of Group Classes

And now, let's visualize the histogram of the dependent variable we are
trying to predict:

```{r}
ggplot(dvs %>% drop_na(), aes(x = BestModel, fill=BestModel)) +
  geom_bar(col="white", alpha=0.5, width = .5) +
  scale_fill_nejm() +
  xlab("Strategy") +
  ylab("Number of Participants") +
  scale_x_discrete(labels=c( "model1" = "Declarative",  "model2" = "Procedural")) +
  ggtitle("Distribution of Strategy Use") +
  theme_pander(lp = "none")

```

Because the classes are not equally distributed, and participants are
more likely to use the memory strategy ($Y=0$) than the procedural one
($Y = 1$), we would need to adjust the weights of our Lasso model.

# Run Model

```{r}

# based on proportion of two labels
W <- Y
W[W == 0] <- mean(Y)
W[W == 1] <- (1-mean(Y))

# normalize maxLL diff
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}


# obtain weights based on the absolute value of maxLL diff between model1 and model2
W <- min_max_norm(abs(dvs.sampling$LL.diff))
 
```


```{r}
USE_TRAIN_TEST_SPLIT <- FALSE
set.seed(0)
  
n = length(Y)
train = sample(1:n, 3*n/4) # by default replace=FALSE in sample() 
test = (1:n)[-train]


if (USE_TRAIN_TEST_SPLIT) {
  
  # find optimal lambda using training dataset
  fit.cv = cv.glmnet(X[train,], Y[train], 
                      alpha=1,
                      weights = W[train],
                      family = "binomial",
                      type.measure = "class",
                      nfolds=length(train),
                      keep=T,
                      standardize=T)
  
  fit =  glmnet(X, Y, 
             alpha = 1, 
             lambda = fit.cv$lambda.min,
             family = "binomial",
             weights = W,
             type.measure = "class",
             standardize=T, 
             grouped = T)
  
} else {
  fit <- glmnet(y = Y,
              x = X,
              alpha=1,
              #lambda = fit.cv$lambda.min,
              weights = W,
              family = "binomial",
              type.measure = "class",
              standardize = T
  )
  
  # LOO CV to find optimal lambda
  fit.cv <- cv.glmnet(y = Y,
                      x = X,
                      alpha=1,
                      family = "binomial",
                      weights = W,
                      type.measure = "class",
                      standardize=T,
                      nfolds=length(Y),
                      grouped = F, 
                      keep = T
                      
  )
}
```

Now, let's look at the cross-validation error profile.

```{r}
plot(fit.cv)
```


### Model Evaluation

```{r}
plot_prediction <- function(Y, Yp, weighted_score, title) {
  wcomparison <- tibble(Observed = Y,
                      Predicted = Yp,
                      DiscretePredicted = ifelse(Yp < 0.5, 0, 1))
              
  wcomparison %<>% mutate(Accuracy = ifelse(DiscretePredicted == Observed,
                                            "Correct", 
                                            "Misclassified")) %>% drop_na()
    
  #rval <- floor(100*cor(Y, Yp))/100 
  aval <- round(100*nrow(dplyr::filter(wcomparison, Accuracy %in% "Correct")) / nrow(wcomparison),2)
  
  # update weighted score
  if (weighted_score > 0) {
    accuracy_score <- weighted_score
  } else {
    accuracy_score <- aval
  }
  
  p <- ggplot(wcomparison, aes(x=Predicted, y=Observed, 
                               col=Accuracy)) +
    geom_point(size=4, alpha=0.6, 
               position= position_jitter(height = 0.02)) +
    geom_abline(intercept = 0, slope = 1, 
                col="red",
                linetype="dashed") +
    scale_color_d3() +
    theme_pander() +
  
    theme(legend.position = "right") +
    guides(col=guide_legend("Classification")) +
    coord_fixed(xlim=c(0, 1), ylim=c(0, 1)) +
    annotate("text", x=0.3, y=0.7,
             label=paste("Accuracy (",
                         length(Y),
                         ") = ",
                         accuracy_score,
                         "%",
                         sep="")) +
    ylab("Observed Strategy") +
    xlab("Predicted Strategy") +
    ggtitle(paste("Predicted vs. Observation",title)) +
    theme(legend.position = "bottom")
    
    ggMarginal(p, 
               fill="grey", 
               alpha=0.75,
               type="density", #bins=13, 
               col="darkgrey",
               margins = "both")
    
    return (p)
}


plot_roc <- function(Y, Yp, weighted_score, title) {
  wcomparison <- tibble(Observed = Y,
                    Predicted = Yp,
                    DiscretePredicted = ifelse(Yp < .5, 0, 1))
            
  wcomparison %<>% mutate(Accuracy = ifelse(DiscretePredicted == Observed,
                                            "Correct", 
                                            "Misclassified")) %>% drop_na()
  wcomparison %<>% mutate(ROCPrediction = if_else(Predicted < .5, 0, 1))

  rocobj <- roc(wcomparison$Observed, wcomparison$ROCPrediction)
  
  if (weighted_score > 0) {
    roc_score <- round(weighted_score, 4)
  } else {
    roc_score <- round(rocobj$auc[[1]], 4)
  }
  

  g <- ggroc(rocobj, col="red") +
    geom_point(aes(y=rocobj$sensitivities, x=rocobj$specificities), col="red", size=4, alpha=.5) +
    ggtitle(paste("AUC ROC Curve", title, roc_score)) +
    xlab("Specificity (FPR)") + ylab("Sensitivity (TPR)") + 
    geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color="grey", linetype="dashed") +
    theme_pander()

  g
}

plot_roc_slide <- function(Y, Yp, title) {
  wcomparison <- tibble(Observed = Y,
                    Predicted = Yp,
                    DiscretePredicted = ifelse(Yp < 0.5, 0, 1))
            
  wcomparison %<>% mutate(Accuracy = ifelse(DiscretePredicted == Observed,
                                            "Correct", 
                                            "Misclassified")) %>% drop_na()
  wcomparison %<>% mutate(ROCPrediction = if_else(Predicted < 0.5, 0, 1))
  curve <- NULL

  for (threshold in seq(0, 1, 0.01)) {
    subthreshold <- wcomparison %>%
      mutate(Prediction = ifelse(Predicted > 1, 1, Predicted)) %>%
      mutate(Prediction = ifelse(Prediction <= 0, 1e-204, Prediction)) %>%
      mutate(Prediction = ifelse(Prediction <= threshold, 0, 1)) %>%
      mutate(Accuracy = ifelse(Prediction == Observed, 1, 0)) %>%
      group_by(Observed) %>%
      summarise(Accuracy = mean(Accuracy))
    
    tnr <- subthreshold %>% 
      filter(Observed == 0) %>% 
      dplyr::select(Accuracy) %>%
      as.numeric()
    
    tpr <- subthreshold %>% 
      filter(Observed == 1) %>% 
      dplyr::select(Accuracy) %>%
      as.numeric()
    
    partial <- tibble(Threshold = threshold,
                      TNR = tnr,
                      TPR = tpr)
    if (is.null(curve)) {
      curve <- partial
    } else {
      curve <- rbind(curve, partial)
    }
  }
  
  s <- ggplot(arrange(curve, TPR), aes(x=TNR, y=TPR)) + 
    geom_point(size=2, col="red", alpha=0.5) + 
    geom_line(col="red") + 
    ylab("Sensitivity (True Positive Rate)") +
    xlab("Specificity (True Negative Rate)") +
    scale_x_reverse() +
    # ylim(0, 1) +
    # xlim(1, 0) +
    ggtitle("ROC Curve for Different Thresholds") +
    geom_abline(slope=1, intercept = 1, col="grey", linetype = "dashed") +
    theme_pander()
  s
}
```

Use assess.glmnet() and confusion.glmnet() to evaluate model performance

```{r}
# evaluate training accuracy
fit.cv.accuracy <- 1-assess.glmnet(fit.cv, 
                                   newx = X[train, ], newy = Y[train],weights = W[train], 
                                   s="lambda.min",  
                                   family = "binomial")$class %>% as.vector()

fit.cv.accuracy_testing <- 1-assess.glmnet(fit.cv, 
                                   newx = X[test, ], newy = Y[test],weights = W[test], 
                                   s="lambda.min",  
                                   family = "binomial")$class %>% as.vector()


fit.cv.auc <- assess.glmnet(fit.cv, 
                                   newx = X[train, ], newy = Y[train],weights = W[train], 
                                   s="lambda.min",  
                                   family = "binomial")$auc %>% as.vector()
fit.cv.auc_testing <- assess.glmnet(fit.cv, 
                                   newx = X[test, ], newy = Y[test],weights = W[test], 
                                   s="lambda.min",  
                                   family = "binomial")$auc %>% as.vector()

# training data prediction probabilities
fit.cv.pred <- predict(fit.cv, newx = X[train, ], 
                       weights = W[train], 
                       s="lambda.min", 
                       type="class", 
                       family = "binomial")%>% as.numeric()


fit.cv.predprob <- predict(fit.cv, 
                           newx = X[train, ], 
                           weights = W[train], 
                           s="lambda.min",
                           type="response", family = "binomial")%>% as.numeric()


```


Calculate training Accuracy score (`r fit.cv.accuracy`) and AUC score
(`r fit.cv.auc`)

Calculate testing Accuracy score (`r fit.cv.accuracy_testing`) and AUC score
(`r fit.cv.auc_testing`)

### Accuracy

> Predicted vs. Observations

```{r}
#score <- (1-assess.glmnet(fit.cv, newx = X , newy = Y ,weights = W, family = "binomial")$class %>% round(4) %>% as.vector())*100
plot_prediction(Y = Y[train], 
                Yp = predict(fit.cv, 
                           newx = X[train, ], 
                           weights = W[train], 
                           s="lambda.min",
                           type="response", family = "binomial")%>% as.numeric(), 
                weighted_score = round(1-assess.glmnet(fit.cv, 
                                   newx = X[train, ], 
                                   newy = Y[train], 
                                   weights = W[train], 
                                   s="lambda.min",  
                                   family = "binomial")$class %>% as.vector(), 4)*100,
                title =  "(Training)")

```


```{r}
plot_prediction(Y[test], 
                Yp = predict(fit.cv, 
                           newx = X[test, ], 
                           weights = W[test], 
                           s="lambda.min",
                           type="response", family = "binomial")%>% as.numeric(), 
                weighted_score = round(1-assess.glmnet(fit.cv, 
                                   newx = X[test, ], 
                                   newy = Y[test], 
                                   weights = W[test], 
                                   s="lambda.min",  
                                   family = "binomial")$class %>% as.vector(), 4)*100,
                title =  "(Testing)")

```
### ROC

> ROC Curve

```{r}
plot_roc(Y = Y[train],
         Yp = predict(fit.cv, 
                       newx = X[train, ], 
                       weights = W[train], 
                       s="lambda.min",
                       type="response", family = "binomial")%>% as.numeric(), 
          weighted_score = round(assess.glmnet(fit.cv, 
                             newx = X[train, ], 
                             newy = Y[train], 
                             weights = W[train], 
                             s="lambda.min",  
                             family = "binomial")$auc %>% as.vector(), 4),
            title =  "(Training)")
```


```{r}
plot_roc(Y = Y[test],
         Yp = predict(fit.cv, 
                       newx = X[test, ], 
                       weights = W[test], 
                       s="lambda.min",
                       type="response", family = "binomial")%>% as.numeric(), 
          weighted_score = round(assess.glmnet(fit.cv, 
                             newx = X[test, ], 
                             newy = Y[test], 
                             weights = W[test], 
                             s="lambda.min",  
                             family = "binomial")$auc %>% as.vector(), 4),
            title =  "(Testing)")
```


> ROC Curve By Sliding Threshold

```{r}
plot_roc_slide(Y = Y[train],
               Yp = predict(fit.cv, 
                       newx = X[train, ], 
                       weights = W[train], 
                       s="lambda.min",
                       type="response", family = "binomial")%>% as.numeric(), 
               title = "Training")
```

