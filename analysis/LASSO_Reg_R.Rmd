---
title: "LASSO Regression R"
date: "`r Sys.Date()`"
author: "Cher Yang"
output:
  html_document:
    code_folding: hide
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(dplyr)
library(magrittr)
library(ggplot2)
library(ggthemes)
library(ppcor)
library(reshape2)
library(groupdata2)
library(knitr)       # kable()

#library(gglasso)
library(glmnet)
library(ggsci)
library(viridis)
library(ggExtra)
library(kableExtra)
library(xtable)
library(ggrepel)
library(scales)
library(car)
library(pROC)
library(patchwork)      # Multi-plot alignment
library(brainconn)
library(igraph)
library(network)
library(brainconn)
library(brainGraph)
library(RColorBrewer)
library(networkD3)
library(ggpubr)
library(DescTools)
library(plotly)
#library(data.table)
library(rsample)   
library(purrr)
library(dplyr)
library(ggplot2)
library(scales)
library(mlbench)
library(kernlab)
library(sessioninfo)
rm(list=ls())
```

**Analysis parameters:**

```{r}
sample_method <- "none"
```

*rsfMRI data Parameters:*

-   rsfMRI: R1S1 mr_pcorr.txt file

-   Z norm before load: False

-   sampling method: `r sample_method`

# Load Data

```{r}
# rest data subjects
rest_subjects = as.data.frame(x  = Sys.glob(paths = "../data/connectivity_matrix/REST1/*")) 
colnames(rest_subjects) <- c('subject_id')  

# clean subject id
rest_subjects <- rest_subjects %>%
  mutate(subject_id = purrr::map_chr(subject_id, ~strsplit(., "/")[[1]][5])) %>%
  mutate(HCPID = paste(str_replace(subject_id, pattern = "sub-", ""), sep = "_", "fnca"))

dvs <- read_csv("../data/LL_model2.csv", #read_csv("../data/actr_maxLL.csv", 
                col_types = cols(
                  .default = col_double(),
                  HCPID = col_character(),
                  best.model = col_character(), 
                  diff.LL = col_double()
                )) %>% 
  # find common subject ID that both rest and task data are available
  inner_join(rest_subjects, by = "HCPID") %>%
  # exclude bad subjects that are hard to distinguish by two models
  #anti_join(bad_subj, by = "HCPID") %>%
  mutate(BestModel = best.model, LL.diff = diff.LL) %>%
  dplyr::select(HCPID, BestModel, LL.diff) %>%
  arrange(HCPID) %>%
  mutate(Y = as.numeric(as.factor(BestModel)) -1,  # model1 = 0, model2 = 1
         HCPID = factor(HCPID), 
         BestModel=factor(BestModel))

```

```{r}
X <- read_csv("../data/X.csv", col_types = cols(.default = col_double()))
Y <- dvs$LL.diff


# based on proportion of two labels
W <- Y
W[W == 0] <- mean(Y)
W[W == 1] <- (1-mean(Y))

# normalize maxLL diff
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}


# obtain weights based on the absolute value of maxLL diff between model1 and model2
W <- min_max_norm(abs(dvs$LL.diff))
```


```{r}
merged_data <- cbind(cbind(X, Y)[, -1], W) %>% mutate(y = Y, y2 = if_else(Y>0, 1, 0))
```

## Sampling


```{r}
set.seed(0) # For reproducibility


if (sample_method == "shuffle") {
  sampled_data <- merged_data
  shuffle_rows <- sample(1:length(rownames(merged_data)))
  sampled_data$y <- merged_data$y[shuffle_rows]
  sampled_data$W <- merged_data$W[shuffle_rows]
} else if (sample_method == "up") {
  sampled_data <- groupdata2::upsample(
    data = merged_data, 
    id_method = "n_rows_c",
    cat_col = "y")
} else if (sample_method == "down") {
  sampled_data <- groupdata2::downsample(
  merged_data,
  id_method = "n_rows_c",
  cat_col = "y")
  
} else if (sample_method == "none") {
  sampled_data <- merged_data
  
} else {
  print("Invalid sample_method. Please choose from: random, up, up_random, none")
}


ggdensity(data = sampled_data, x = "Y", fill = "steelblue") + 
  ggtitle("Histogram of DV (Log-Likelihood Differences)") 

```


```{r}
Y.sampled <- sampled_data$y
W.sampled <- sampled_data$W
X.sampled <- sampled_data[, !names(sampled_data) %in% c("y", "W", "y2", "Y")]
```

## Train/Test Split

```{r}
set.seed(0)  # Set seed for reproducibility

indices <- 1:nrow(X.sampled)

# Sample 80% of the indices for training
train_indices <- sample(indices, 0.8 * length(indices), replace = FALSE)

# Create training and testing sets
X_train <- X.sampled[train_indices, ]
Y_train <- Y.sampled[train_indices]
W_train <- W.sampled[train_indices]

X_test <- X.sampled[-train_indices, ]
Y_test <- Y.sampled[-train_indices]
W_test <- W.sampled[-train_indices]

```

> Quality and Characteristics of $X$ and $Y$

Let's do some visualization and analysis of our indepedenent and
dependet variables, just to ensure there are no obvious problems.

> Collinearity of Connectivity Regressors $X$

The regressors $X$ is certainly multi-collinear; that is a consequence
of having a large number of predictors $p > n$, which, in turn, is one
of the reasons why we are using Lasso. Too much collinearity, however,
could be really bad and push Lasso towards selecting non-optimal
regressors. To gather a sense of how much collinearity we have, we can
plot the distribution of correlations among regressors:

```{r}
corX <- cor(X)
distCor <- as_vector(corX[lower.tri(corX, diag = F)])
distTibble <- as_tibble(data.frame(R=distCor))

ggplot(distTibble, aes(x=R)) +
  geom_histogram(col="white", alpha=0.5, binwidth = 0.05) +
  theme_pander() +
  ylab("Number of Correlations") +
  xlab("Correlation Value") +
  ggtitle("Distribution of Correlation Values Between Regressors")
```

# Run LASSO Model

LOO on training data, evaluate on testing data


```{r}

  # LOO CV to find optimal lambda
  fit.cv <- cv.glmnet(y = Y_train,
                      x = as.matrix(X_train),
                      alpha=1,
                      type.measure = "mse",
                      standardize=T,
                      nfolds=length(Y_train),
                      grouped = F, 
                      keep = T)

  best_model <- glmnet(y = Y_train,
              x = as.matrix(X_train),
              alpha=1,
              lambda = fit.cv$lambda.min,
              type.measure = "mse",
              standardize = T)
  
  best_model.fulldataset <- glmnet(y = Y.sampled,
              x = as.matrix(X.sampled),
              alpha=1,
              lambda = fit.cv$lambda.min,
              type.measure = "mse",
              standardize = T)

```

Now, let's look at the cross-validation error profile.

```{r}
plot(fit.cv)
```
```{r}
# Access the optimal lambda and its associated MSE value
optimal_lambda <- fit.cv$lambda.min
optimal_mse <- fit.cv$cvm[fit.cv$lambda == optimal_lambda]

# Display the optimal MSE value
print(paste("Optimal Lambda:", optimal_lambda))
print(paste("Corresponding Optimal MSE:", optimal_mse))

```


### Training Datasets Betas


```{r}
#coef(fit.cv, s = "lambda.min")
betas <- as.matrix(best_model$beta) 
print(paste("[Training dataset] Non-Zero Betas:", sum(betas != 0)))

ggdensity(data = data.frame(betas) %>% filter(s0 != 0), 
          x="s0", 
          fill="forestgreen") +
  labs(x = "Coefficients", y="Values") +
  ggtitle(paste("Non-Zero Coefficients from Training Datasets", "[ N =",sum(betas != 0), "]"))

```

We save betas from training datasets

```{r}
write.csv(betas, file = "../data/B.csv")
```



### Full Datasets Betas



```{r}
betas <- as.matrix(best_model.fulldataset$beta) 
print(paste("[Full dataset] Non-Zero Betas:", sum(betas != 0)))

ggdensity(data = data.frame(betas) %>% filter(s0 != 0), 
          x="s0", 
          fill="forestgreen") +
  labs(x = "Coefficients", y="Values") +
  ggtitle(paste("Non-Zero Coefficients from Full Datasets", "[ N =",sum(betas != 0), "]"))

```

# Evaluate LASSO Model

## Training performance 

```{r}
# Predict on the test set using the best model 
plot_training_data <- data.frame(i = 1:length(Y_train),
                        Observed = Y_train,
                        Predicted = predict(best_model, newx = as.matrix(X_train)) %>% as.vector()) 

mse <- round(mean((plot_training_data$Observed - plot_training_data$Predicted)^2), 4)



ggplot(data = plot_training_data, aes(x = i)) + 
  geom_point(aes(y = Observed), color = "steelblue", shape = "o", size = 5) +
  geom_point(aes(y = Predicted), color = "tomato", shape = "x", size = 5) +
  labs(x = "Subject ID (Training)", y = "Predicted Y", 
       title = paste("Training: Predicted vs. Observed [ MSE = ", mse, "]")) +
  scale_shape_manual(name = "Y values", values = c("Observed" = "o", "Predicted" = "x"),
                     labels = c("Observed", "Predicted")) +
  theme_pander()


```

```{r}
ggplot(plot_training_data, aes(x = Observed, y = Predicted)) +
  geom_point(color = "black", size = 5, alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed") +
  labs(x = "Observed", y = "Predicted", 
       title = "Training: Predicted vs. Observed") +
  xlim(0, max(plot_training_data$Observed)) +
  ylim(0, max(plot_training_data$Observed))+
  theme_pander()

```

## Testing performance 


```{r}
# Predict on the test set using the best model 
plot_testing_data <- data.frame(i = 1:length(Y_test),
                        Observed = Y_test,
                        Predicted = predict(best_model, newx = as.matrix(X_test)) %>% as.vector()) 

mse <- round(mean((plot_testing_data$Observed - plot_testing_data$Predicted)^2), 4)



ggplot(data = plot_testing_data, aes(x = i)) + 
  geom_point(aes(y = Observed), color = "steelblue", shape = "o", size = 5) +
  geom_point(aes(y = Predicted), color = "tomato", shape = "x", size = 5) +
  labs(x = "Subject ID (Testing)", y = "Predicted Y", 
       title = paste("Testing: Predicted vs. Observed [ MSE = ", mse, "]")) +
  scale_shape_manual(name = "Y values", values = c("Observed" = "o", "Predicted" = "x"),
                     labels = c("Observed", "Predicted")) +
  theme_pander()
```

```{r} 
# Plot predicted vs. observed using ggplot2
ggplot(plot_testing_data, aes(x = Observed, y = Predicted)) +
  geom_point(color = "black", size = 5, alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed") +
  labs(x = "Observed", y = "Predicted", 
       title = "Testing: Predicted vs. Observed") +
  xlim(0, max(plot_testing_data$Observed)) +
  ylim(0, max(plot_testing_data$Observed)) +
  theme_pander()
```


# Nested CV

```{r}

set.seed(0)  # Set seed for reproducibility

# Function to perform NCV
perform_nested_cv <- function(X, Y, W, N = 20) {
  indices <- 1:nrow(X)
  mse_scores <- numeric(N)
  
  for (i in 1:N) {
    # Sample indices for inner loop
    inner_indices <- sample(indices, 0.8 * length(indices), replace = FALSE)
    
    # Create inner training and validation sets
    X_inner_train <- X[inner_indices, ]
    Y_inner_train <- Y[inner_indices]
    
    X_inner_val <- X[-inner_indices, ]
    Y_inner_val <- Y[-inner_indices]
    
    # Perform model selection using inner loop CV
    fit.cv <- cv.glmnet(y = Y_inner_train,
                        x = as.matrix(X_inner_train),
                        alpha = 1,
                        type.measure = "mse",
                        standardize = TRUE,
                        nfolds = length(Y_inner_train),
                        grouped = FALSE,
                        keep = TRUE)
    
    # Train model on full inner training set with optimal lambda
    best_model <- glmnet(y = Y_inner_train,
                         x = as.matrix(X_inner_train),
                         alpha = 1,
                         lambda = fit.cv$lambda.min,
                         type.measure = "mse",
                         standardize = TRUE)
    
    # Evaluate model on inner validation set
    mse_scores[i] <- mean((predict(best_model, newx = as.matrix(X_inner_val)) - Y_inner_val) ^ 2)
  }
  
  # Get the index of the best model based on MSE
  best_model_index <- which.min(mse_scores)
  
  # Create training and testing sets for outer loop using the same method as before
  outer_indices <- sample(indices, 0.8 * length(indices), replace = FALSE)
  
  X_outer_train <- X[outer_indices, ]
  Y_outer_train <- Y[outer_indices]
  W_outer_train <- W[outer_indices]
  
  X_outer_test <- X[-outer_indices, ]
  Y_outer_test <- Y[-outer_indices]
  W_outer_test <- W[-outer_indices]
  
  # Train final model on full outer training set with the optimal lambda from inner loop
  final_model <- glmnet(y = Y_outer_train,
                        x = as.matrix(X_outer_train),
                        alpha = 1,
                        lambda = fit.cv$lambda.min,
                        type.measure = "mse",
                        standardize = TRUE)
  
  return(list(best_model = final_model, full_dataset_model = best_model.fulldataset))
}

# Call the function with your X.sampled, Y.sampled, and W.sampled data
result <- perform_nested_cv(X.sampled, Y.sampled, W.sampled, N = 20)

# Access the best model and full dataset model from the result
best_model <- result$best_model
full_dataset_model <- result$full_dataset_model

```


```{r} 

set.seed(0)  # Set seed for reproducibility

# Function to perform NCV and plot results
perform_nested_cv_and_plot <- function(X, Y, W, N = 20) {
  indices <- 1:nrow(X)
  mse_scores <- numeric(N)
  optimal_lambdas <- numeric(N)
  
  for (i in 1:N) {
    # Sample indices for inner loop
    inner_indices <- sample(indices, 0.8 * length(indices), replace = FALSE)
    
    # Create inner training and validation sets
    X_inner_train <- X[inner_indices, ]
    Y_inner_train <- Y[inner_indices]
    
    X_inner_val <- X[-inner_indices, ]
    Y_inner_val <- Y[-inner_indices]
    
    # Perform model selection using inner loop CV
    fit.cv <- cv.glmnet(y = Y_inner_train,
                        x = as.matrix(X_inner_train),
                        alpha = 1,
                        type.measure = "mse",
                        standardize = TRUE,
                        nfolds = length(Y_inner_train),
                        grouped = FALSE,
                        keep = TRUE)
    
    # Record the optimal lambda for the inner fold
    optimal_lambdas[i] <- fit.cv$lambda.min
    
    # Train model on full inner training set with optimal lambda
    best_model <- glmnet(y = Y_inner_train,
                         x = as.matrix(X_inner_train),
                         alpha = 1,
                         lambda = fit.cv$lambda.min,
                         type.measure = "mse",
                         standardize = TRUE)
    
    # Evaluate model on inner validation set
    mse_scores[i] <- mean((predict(best_model, newx = as.matrix(X_inner_val)) - Y_inner_val) ^ 2)
  }
  
  # Get the index of the best model based on MSE
  best_model_index <- which.min(mse_scores)
  final_optimal_lambda <- optimal_lambdas[best_model_index]
  
  # Train final model on full outer training set with the optimal lambda from inner loop
  final_model <- glmnet(y = Y,
                        x = as.matrix(X),
                        alpha = 1,
                        lambda = final_optimal_lambda,
                        type.measure = "mse",
                        standardize = TRUE)
  
  # Predict on the test set using the best model
  plot_testing_data <- data.frame(i = 1:length(Y),
                                  Observed = Y,
                                  Predicted = predict(final_model, newx = as.matrix(X)) %>% as.vector()) 
  
  mse <- round(mean((plot_testing_data$Observed - plot_testing_data$Predicted)^2), 4)
  
  # Plot distribution of optimal lambdas
  ggplot() +
    geom_histogram(aes(x = optimal_lambdas), bins = 10, fill = "gold", color = "black") +
    labs(x = "Optimal Lambdas", y = "Frequency",
         title = "Distribution of Optimal Lambdas from Nested CV") +
    theme_minimal()
  
  # Plot observation against prediction
  ggplot(data = plot_testing_data, aes(x = i)) + 
    geom_point(aes(y = Observed), color = "steelblue", shape = "o", size = 5) +
    geom_point(aes(y = Predicted), color = "tomato", shape = "x", size = 5) +
    labs(x = "Subject ID (Testing)", y = "Predicted Y", 
         title = paste("Testing: Predicted vs. Observed [ MSE = ", mse, "]")) +
    scale_shape_manual(name = "Y values", values = c("Observed" = "o", "Predicted" = "x"),
                       labels = c("Observed", "Predicted")) +
    theme_minimal()
}

# Call the function with your X.sampled, Y.sampled, and W.sampled data
perform_nested_cv_and_plot(X.sampled, Y.sampled, W.sampled, N = 20)

```
