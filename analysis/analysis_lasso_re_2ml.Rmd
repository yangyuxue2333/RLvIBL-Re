---
title: "Analysis Lasso-Re (ML)"
date: "`r Sys.Date()`"
author: "Cher Yang"
output:
  html_document:
    code_folding: hide
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(dplyr)
library(magrittr)
library(ggplot2)
library(ggthemes)
library(ppcor)
library(reshape2)
#library(gglasso)
library(glmnet)
library(ggsci)
library(viridis)
library(ggExtra)
library(kableExtra)
library(xtable)
library(ggrepel)
library(scales)
library(car)
library(pROC)
library(patchwork)      # Multi-plot alignment
library(brainconn)
library(igraph)
library(network)
library(brainconn)
library(brainGraph)
library(RColorBrewer)
library(networkD3)
library(ggpubr)
library(DescTools)
library(plotly)
#library(data.table)
library(rsample)   
library(purrr)
library(dplyr)
library(ggplot2)
library(scales)
library(mlbench)
library(kernlab)
library(sessioninfo)
rm(list=ls())
```


**Analysis parameters:**

*rsfMRI data Parameters:*

-   rsfMRI: R1S1 mr_pcorr.txt file

-   Z norm before load: False

-   upsampling: **UP**

*ML Parameters:* - Z norm in glmnet: True

-   W: **normalized `maxLL_difference` (min max normalization)**

-   Hyperparameter tuning: ~~nfolds = length(Y)~~ best lambda =
    median(nested CV lambdas)

-   Model evaluation: averaged accuracy/acu_roc score of nested CV

*Nested CV Parameters*

-   ~~Betas: \|mean beta\| \> 0.05~~

-   Round = 200, CV folds = 20

-   Train/Test Split: 1:3, Seed=0

-   Lambda: ~~1se~~ median of min lambdas from 200 rounds


# Load Data

Load intermediate data after running `analysis_lasso_re_preapre_data.Rmd`

```{r}
load(file = "../data/__cache__/worksapce_prepare_data.RData")
```

# Machine-Learning with Lasso

## Regular Cross-Validation

We can now define the lasso model. We will use the elastic net approach
with $\alpha = 0$ to generate a pure lasso model. The model will use a
binomial (i.e., logistic) regression and will measure the
cross-validation error as class misalignment.

To analyze the data, we will use Lasso, a statistical learning system
based on penalized regression.

Most of the entries in our $Y$ vector are coded as "0" (i.e., most
participants use the memory strategy), which creates an imbalance. We
are going to create an appropriate set of weights so that the two
classes are balanced.

New: Instead of using proportion as weights, we apply maxLL_difference
as weight to increase importance of individual data who has larger
maxLL_difference.

```{r}

# based on proportion of two labels
W <- Y
W[W == 0] <- mean(Y)
W[W == 1] <- (1-mean(Y))

# normalize maxLL diff
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}


# obtain weights based on the absolute value of maxLL diff between model1 and model2
W <- min_max_norm(abs(dvs.up$LL.diff))
 
```

To choose the optimal value of $\lambda$ in Lasso, we will examine the
cross-validation error during a LOO cross-validation.

Alternatively, if we split dataset into train/test, whether we could
effectively predict? It turns out that the testing accuracy for unseen
data is bad (0.63).

It's better to pursue consensus feature approach get betas

```{r}
USE_TRAIN_TEST_SPLIT <- FALSE
set.seed(0)
  
n = length(Y)
train = sample(1:n, 3*n/4) # by default replace=FALSE in sample() 
test = (1:n)[-train]


if (USE_TRAIN_TEST_SPLIT) {
  
  # find optimal lambda using training dataset
  fit.cv = cv.glmnet(X[train,], Y[train], 
                      alpha=1,
                      weights = W[train],
                      family = "binomial",
                      type.measure = "class",
                      nfolds=length(train),
                      keep=T,
                      standardize=T)
  
  fit =  glmnet(X, Y, 
             alpha = 1, 
             lambda = fit.cv$lambda.min,
             family = "binomial",
             weights = W,
             type.measure = "class",
             standardize=T, 
             grouped = T)
  
} else {
  fit <- glmnet(y = Y,
              x = X,
              alpha=1,
              #lambda = fit.cv$lambda.min,
              weights = W,
              family = "binomial",
              type.measure = "class",
              standardize = T
  )
  
  # LOO CV to find optimal lambda
  fit.cv <- cv.glmnet(y = Y,
                      x = X,
                      alpha=1,
                      family = "binomial",
                      weights = W,
                      type.measure = "class",
                      standardize=T,
                      nfolds=length(Y),
                      grouped = F, 
                      keep = T
                      
  )
}
```

Now, let's look at the cross-validation error profile.

```{r}
plot(fit.cv)
```

The profile has the characteristic U-shape or increasing curve, with
more error as $\lambda$ increases. ~~As recommended by Tibishirani, we
will select the "lambda 1SE" value, which is the largest~~ $\lambda$
value that does not differ by more tha 1 SE from the $\lambda$ value
that gives us the minimum cross validation error. This guarantees the
maximum generalizability.

We will select the "lambda min" value, leaving approximately 68 non-zero
connections.

We can also visualize the profile of the beta weights of each connection
for different values of $\lambda$.

```{r}
plot(fit, sub="Beta Values for Connectivity") 
L1norm <- sum(abs(fit$beta[,which(fit$lambda==fit.cv$lambda.min)]))
abline(v=L1norm, lwd=2, lty=2) 
```

And now, plot prettier version

```{r}
lasso_df <- as_tibble(data.frame(lambda=fit.cv$lambda, 
                                 error=fit.cv$cvm, 
                                 sd=fit.cv$cvsd))

ggplot(lasso_df, aes(x=lambda, y=error)) +
  geom_line(aes(col=error), lwd=2) +
  scale_color_viridis("Error", option = "plasma") +
  geom_ribbon(aes(ymin=error -sd, ymax=error + sd), alpha=0.2,fill="blue") +
  xlab(expression(lambda)) +
  ylab("Cross-Validation Error") +
  ggtitle(expression(paste(bold("Cross Validation Error Across "), lambda))) +
  geom_vline(xintercept = lasso_df$lambda[lasso_df$error==min(lasso_df$error)]) +
  theme_pander() +
  theme(legend.position="right")
```

The min $\lambda_{min}$ is `r fit.cv$lambda.min`, The 1se min
$\lambda_{min}$ is `r fit.cv$lambda.1se`

### Model Evaluation

```{r}
plot_prediction <- function(Y, Yp, weighted_score, title) {
  wcomparison <- tibble(Observed = Y,
                      Predicted = Yp,
                      DiscretePredicted = ifelse(Yp < 0.5, 0, 1))
              
  wcomparison %<>% mutate(Accuracy = ifelse(DiscretePredicted == Observed,
                                            "Correct", 
                                            "Misclassified")) %>% drop_na()
    
  #rval <- floor(100*cor(Y, Yp))/100 
  aval <- round(100*nrow(dplyr::filter(wcomparison, Accuracy %in% "Correct")) / nrow(wcomparison),2)
  
  # update weighted score
  if (weighted_score > 0) {
    accuracy_score <- weighted_score
  } else {
    accuracy_score <- aval
  }
  
  p <- ggplot(wcomparison, aes(x=Predicted, y=Observed, 
                               col=Accuracy)) +
    geom_point(size=4, alpha=0.6, 
               position= position_jitter(height = 0.02)) +
    geom_abline(intercept = 0, slope = 1, 
                col="red",
                linetype="dashed") +
    scale_color_d3() +
    theme_pander() +
  
    theme(legend.position = "right") +
    guides(col=guide_legend("Classification")) +
    coord_fixed(xlim=c(0, 1), ylim=c(0, 1)) +
    annotate("text", x=0.3, y=0.7,
             label=paste("Accuracy (",
                         length(Y),
                         ") = ",
                         accuracy_score,
                         "%",
                         sep="")) +
    ylab("Observed Strategy") +
    xlab("Predicted Strategy") +
    ggtitle(paste("Predicted vs. Observation",title)) +
    theme(legend.position = "bottom")
    
    ggMarginal(p, 
               fill="grey", 
               alpha=0.75,
               type="density", #bins=13, 
               col="darkgrey",
               margins = "both")
    
    return (p)
}


plot_roc <- function(Y, Yp, weighted_score, title) {
  wcomparison <- tibble(Observed = Y,
                    Predicted = Yp,
                    DiscretePredicted = ifelse(Yp < 0.5, 0, 1))
            
  wcomparison %<>% mutate(Accuracy = ifelse(DiscretePredicted == Observed,
                                            "Correct", 
                                            "Misclassified")) %>% drop_na()
  wcomparison %<>% mutate(ROCPrediction = if_else(Predicted < 0.5, 0, 1))

  rocobj <- roc(wcomparison$Observed, wcomparison$ROCPrediction)
  
  if (weighted_score > 0) {
    roc_score <- round(weighted_score, 4)
  } else {
    roc_score <- round(rocobj$auc[[1]], 4)
  }
  

  g <- ggroc(rocobj, col="red") +
    geom_point(aes(y=rocobj$sensitivities, x=rocobj$specificities), col="red", size=4, alpha=.5) +
    ggtitle(paste("AUC ROC Curve", title, roc_score)) +
    xlab("Specificity (FPR)") + ylab("Sensitivity (TPR)") + 
    geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color="grey", linetype="dashed") +
    theme_pander()
  
  g
}

plot_roc_slide <- function(Y, Yp, title) {
  wcomparison <- tibble(Observed = Y,
                    Predicted = Yp,
                    DiscretePredicted = ifelse(Yp < 0.5, 0, 1))
            
  wcomparison %<>% mutate(Accuracy = ifelse(DiscretePredicted == Observed,
                                            "Correct", 
                                            "Misclassified")) %>% drop_na()
  wcomparison %<>% mutate(ROCPrediction = if_else(Predicted < 0.5, 0, 1))
  curve <- NULL

  for (threshold in seq(0, 1, 0.01)) {
    subthreshold <- wcomparison %>%
      mutate(Prediction = ifelse(Predicted > 1, 1, Predicted)) %>%
      mutate(Prediction = ifelse(Prediction <= 0, 1e-204, Prediction)) %>%
      mutate(Prediction = ifelse(Prediction <= threshold, 0, 1)) %>%
      mutate(Accuracy = ifelse(Prediction == Observed, 1, 0)) %>%
      group_by(Observed) %>%
      summarise(Accuracy = mean(Accuracy))
    
    tnr <- subthreshold %>% 
      filter(Observed == 0) %>% 
      dplyr::select(Accuracy) %>%
      as.numeric()
    
    tpr <- subthreshold %>% 
      filter(Observed == 1) %>% 
      dplyr::select(Accuracy) %>%
      as.numeric()
    
    partial <- tibble(Threshold = threshold,
                      TNR = tnr,
                      TPR = tpr)
    if (is.null(curve)) {
      curve <- partial
    } else {
      curve <- rbind(curve, partial)
    }
  }
  
  s <- ggplot(arrange(curve, TPR), aes(x=TNR, y=TPR)) + 
    geom_point(size=2, col="red", alpha=0.5) + 
    geom_line(col="red") + 
    ylab("Sensitivity (True Positive Rate)") +
    xlab("Specificity (True Negative Rate)") +
    scale_x_reverse() +
    # ylim(0, 1) +
    # xlim(1, 0) +
    ggtitle("ROC Curve for Different Thresholds") +
    geom_abline(slope=1, intercept = 1, col="grey", linetype = "dashed") +
    theme_pander()
  s
}
```

Use assess.glmnet() and confusion.glmnet() to evaluate model performance

```{r}
# evaluate training accuracy
fit.cv.accuracy <- 1-assess.glmnet(fit.cv, 
                                   newx = X[train, ], newy = Y[train],weights = W[train], 
                                   s="lambda.min",  
                                   family = "binomial")$class %>% as.vector()

fit.cv.accuracy_testing <- 1-assess.glmnet(fit.cv, 
                                   newx = X[test, ], newy = Y[test],weights = W[test], 
                                   s="lambda.min",  
                                   family = "binomial")$class %>% as.vector()


fit.cv.auc <- assess.glmnet(fit.cv, 
                                   newx = X[train, ], newy = Y[train],weights = W[train], 
                                   s="lambda.min",  
                                   family = "binomial")$auc %>% as.vector()
fit.cv.auc_testing <- assess.glmnet(fit.cv, 
                                   newx = X[test, ], newy = Y[test],weights = W[test], 
                                   s="lambda.min",  
                                   family = "binomial")$auc %>% as.vector()

# training data prediction probabilities
fit.cv.pred <- predict(fit.cv, newx = X[train, ], 
                       weights = W[train], 
                       s="lambda.min", 
                       type="class", 
                       family = "binomial")%>% as.numeric()


fit.cv.predprob <- predict(fit.cv, 
                           newx = X[train, ], 
                           weights = W[train], 
                           s="lambda.min",
                           type="response", family = "binomial")%>% as.numeric()


```

```{r eval=F, include=F}
# validation data misclassification error

#training 
assess.glmnet(fit, newx = X[train,], newy = Y[train])
training_preds = predict(fit, newx = X[train,])
assess.glmnet(training_preds, newy = Y[train], family = "binomial")


#testing accuracy
fit.cv.accuracy <- 1-assess.glmnet(fit.cv, X[test, ], Y[test], 
                                   weights = W[test], 
                                   s="lambda.min", #"lambda.1se", 
                                   family = "binomial")$class %>% as.vector()# best lambda cv error

fit.predict <- predict(fit, newx = X[-train, ], s = c(1, 0.25))

fit.predprob <- predict(fit,  newx = X[-train, ], weights = W,  s="lambda.min", type="response", family = "binomial")%>% as.numeric()
```

```{r eval=F, include=F}
# validation data misclassification error

#training
fit.cv.accuracy <- 1-assess.glmnet(fit.cv, X, Y, 
                                   weights = W, 
                                   s="lambda.min", #"lambda.1se", 
                                   family = "binomial")$class %>% as.vector()# best lambda cv error


fit.cv.auc <- assess.glmnet(fit.cv, X, Y, 
                            weights = W, 
                            s="lambda.min", 
                            family = "binomial")$auc %>% as.vector()# best lambda cv error
  
# training data prediction probabilities
fit.cv.pred <- predict(fit.cv, newx = X[test], 
                       weights = W[test], 
                       s="lambda.min", 
                       type="class", 
                       family = "binomial")%>% as.numeric()


fit.cv.predprob <- predict(fit.cv, 
                           newx = X, 
                           weights = W, 
                           s="lambda.min",
                           type="response", family = "binomial")%>% as.numeric()

```

Calculate training Accuracy score (`r fit.cv.accuracy`) and AUC score
(`r fit.cv.auc`)

### Accuracy

> Predicted vs. Observations

```{r}
score <- round(1-as.vector(assess.glmnet(fit.cv, X, Y, W, "binomial")$class), 4)
plot_prediction(Y[train], fit.cv.predprob, score, "(Training)")
```

### ROC

> ROC Curve

```{r}
score <- round(as.vector(assess.glmnet(fit.cv, X, Y, W, "binomial")$auc), 4)
plot_roc(Y[train], fit.cv.predprob, score, "Training")
```

> ROC Curve By Sliding Threshold

```{r}
plot_roc_slide(Y[train], fit.cv.predprob, "Training")
```

### Connectome

Let's have a better look at the relevant connections that survive the
Lass penalty at the value of $\lambda_{min} + 1 SE$. We start by saving
these connections in a tibble, and saving the data on a file for later
use.

```{r}
#betas <- fit$beta[, which(fit$lambda==fit.cv$lambda.1se)]
betas <- fit$beta[, which(fit$lambda==fit.cv$lambda.min)]
conn_betas <- as_tibble(data.frame(index=I$index, Beta=betas))
connectome <- order %>%
  filter(index %in% I$index) %>%
  inner_join(conn_betas) %>%
  dplyr::select(-censor2) %>%
  filter(Beta != 0) %>%
  
  # reformat connectome
  separate(connection, c("connection1", "connection2"))%>%
  separate(network, sep = "-", c("network1", "network2"), remove = F) %>%
  #filter(str_detect(network, pattern = "-1-")) %>%
  mutate(network1 = ifelse(str_detect(network, pattern = "-1-"), -1, network1)) %>%
  mutate(connection_type = ifelse(network1==network2, "Within", "Between")) %>%
  arrange(index)


# HARD CODE
connectome[connectome$network=="-1-5","network2"] <- "5"
connectome[connectome$network=="-1-7","network2"] <- "7"
connectome[connectome$network=="-1--1","network2"] <- "-1"
connectome[connectome$network=="-1-11","network2"] <- "11"
connectome[connectome$network=="-1-12","network2"] <- "12"
connectome[connectome$network=="-1-13","network2"] <- "13"

```

In sum, connectome has `r dim(connectome)[[1]]` non-zero connections,
`r sum(connectome$Beta>0)` positive beta, and `r sum(connectome$Beta<0)`
negative betas. We will use these betas for later brain connectivity
analysis.

Finally, we can visualize the table of connections

```{r}
connectome %>%
  xtable() %>%
  kable(digits = 5) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

### Stability of Estimated Beta Weights

And now, let's visualize the beta weights of the connections: num
connections=`r dim(connectome)[[1]]`

```{r, fig.width=10, fig.height=10, eval=FALSE, include=FALSE}

ggplot(connectome, aes(x = reorder(connection, Beta), y = Beta)) +
  geom_point(aes(col=Beta), alpha=.5, 
             size=2,
             position = position_jitter(height = 0, width = 0.3)) +
  stat_summary(fun.data = "mean_sdl", geom="point", fill="black", alpha=1, size=1) +
  scale_color_gradient2(low = "dodgerblue",
                        mid = "wheat",
                        high = "red2",
                        midpoint = 0) +
  scale_x_discrete(labels = 
                     paste(connectome$network_names, 
                           " (", 
                           connectome$connection,
                           ")", sep="")) +
  ggtitle(paste("Connection Weights Across Cross-Validation:", dim(connectome)[[1]])) +
  ylab(expression(paste(beta, " value"))) +
  xlab("Connection") +
  geom_hline(yintercept = 0, col="grey") +
  stat_summary(fun.data = "mean_cl_boot", 
               col="black", geom="errorbar", width=1) +
  scale_color_viridis(option="plasma", begin=0.2, end=0.9) +
  theme_pander() +
  theme(axis.text.y = element_text(angle=0, hjust=1),
        legend.position = "NA") +
  #ylim(-3, 3) +
  coord_flip()
```

```{r}
connectome %>% 
  mutate(beta_sign = ifelse(Beta >0, "+", "-")) %>%
  ggdotchart(x = "network_names", y = "Beta",
           color = "beta_sign",                                # Color by groups
           palette = c("steelblue", "tomato"), # Custom color palette
           rotate = TRUE,
           facet.by = "connection_type", 
           sort.by.groups = F,
           sort.val = "desc",          # Sort the value in descending order
           sorting = "descending",                       # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           add.params = list(color = "lightgray", size = 2), # Change segment color and size
           group = "connection_type",                                # Order by groups
           dot.size = 3,                                 # Large dot size
           #label = round(connectome$Beta,2),                        # Add mpg values as dot labels
           #font.label = list(color = "white", size = 9,
           #                  vjust = 0.5),               # Adjust label parameters
           #group = "cyl",
           #y.text.col = TRUE,
           title = paste("Lasso Connection Weights:", dim(connectome)[[1]]),
           ggtheme = theme_pander()) +
  geom_hline(yintercept = 0, linetype = 2, color = "black")
   
```

### Statistical analysis of Network Distribution

Lasso vs. Power

```{r}
subsetPower <- filter(power2011,
                      NetworkName %in% NOI)
noi_stats <- subsetPower %>%
  group_by(NetworkName, Color) %>%
  summarise(N=length(Color)/dim(subsetPower)[1]) %>%
  add_column(Source="Power")

lROIs <- unique(c(connectome$connection1, connectome$connection2))

rois <- power2011[lROIs,]
roi_stats <- rois %>%
  group_by(NetworkName, Color, .drop = F) %>%
  summarise(N=length(Color)/length(lROIs)) %>%
  add_column(Source="Lasso") 


roi_stats <- roi_stats %>% 
  right_join(noi_stats %>% dplyr::select(NetworkName, Color), on = c("NetworkName", "Color")) %>%
  mutate(N = ifelse(is.na(N), 0, N), Source="Lasso") %>%
  arrange(NetworkName)

total_stats <- rbind(noi_stats, roi_stats)
```

```{r}
ggplot(total_stats, aes(x="", y=N, fill=NetworkName)) +
  geom_bar(stat = "identity", col="white", width=1) +
  facet_grid(~Source, labeller = label_both) +
  coord_polar("y", start=0) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 2L)) +
  scale_fill_manual(values = unique(power2011$Color)) +
  #scale_fill_viridis(discrete = T) +
  #scale_fill_ucscgb() +
  ylab("") +
  xlab("") +
  ggtitle("Distriution of ROI") +
  geom_text_repel(aes(label=percent(N, .1)), col="black", 
            position=position_stack(vjust=.01), size=3)+
  theme_pander() +
  guides(fill=guide_legend(ncol=2)) +
  theme(legend.position = "bottom")

#ggbarplot(total_stats, x="NetworkName", y="N", facet.by = "Source", fill = "NetworkName", color = "white",
#          merge = T, label = F,
#          ) +
#  coord_polar("y", start=0) 
```

```{r}
chisq.test(roi_stats$N*length(lROIs), p = noi_stats$N)
```

Lasso vs. Power:

Between vs. Within

```{r}
net_from <- function(s) {as.character(strsplit(s, "-")[[1]][1])}
net_to <- function(s) {as.character(strsplit(s, "-")[[1]][2])}

vnet_from <- Vectorize(net_from)
vnet_to <- Vectorize(net_to)

connectivity <- function(s) {
  if (net_from(s) == net_to(s)) {
    "Within"
  } else {
    "Between"
  }
}

vconnectivity <- Vectorize(connectivity)
coi <- order %>%
  filter(censor == TRUE) %>%
  filter(network_names %in% COI) 

coi$from <- vnet_from(coi$network_names)
coi$to <- vnet_to(coi$network_names)
coi$connection_type <- vconnectivity(coi$network_names)

coi_stats <- coi %>% 
  group_by(connection_type) %>% 
  summarise(N=length(index), P=length(index)/dim(coi)[1]) %>%
  add_column(Source = "Power et al. (2011)")
```

```{r}
connectome$connection_type <- vconnectivity(connectome$network_names)
connectome_stats <- connectome %>%
  group_by(connection_type) %>% 
  summarise(N=length(index), P=length(index)/dim(connectome)[1]) %>%
  add_column(Source = "Lasso Analysis")

connect_stats <- rbind(connectome_stats, coi_stats)
```

```{r}
ggplot(connect_stats, aes(x="", y=P, fill=connection_type)) +
  geom_bar(stat = "identity", col="grey", width=1) +
  facet_grid(~Source, labeller = label_both) +
  coord_polar("y", start=0) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 2L)) +
  scale_fill_jama() +
  scale_color_jama() +
  ylab("") +
  xlab("") +
  ggtitle("Distribuction of Connectivity\n(Within/Between Networks)") +
  geom_text_repel(aes(label=percent(P, .1)), col="white",
            position=position_stack(vjust=1), size=3)+
  theme_pander() +
  theme(legend.position = "bottom")
```

```{r}
    chisq.test(connectome_stats$N, p=coi_stats$P)
```

## Nested Cross-Validation

In nested CV, lambda is not same for each subject

```{r eval=FALSE, include=FALSE }
# Andrea's method of nested cross-validation (deprecated)

N <- length(Y)
P <- ncol(X)
betas <- matrix(rep(0, P * N), nrow = N)
Yp <- rep(0, N)
minF = 1
#X <- atanh(X)  ## ??? WHY USE atanh
dfX <- as.data.frame(cbind(Y, X))
for (i in 1:N) {
  Ytrain <- Y[-i]
  Xtrain <- X[-i,]
  Wtrain <- W[-i]
  # fit <- ncvreg
  fit <- glmnet(y = Ytrain,
                x = Xtrain,
                weights = Wtrain,
                alpha = 1,
                family = "binomial",
                type.measure ="class",
                standardize = T
  )
  
  fit.cv <- cv.glmnet(y = Ytrain,
                      x = Xtrain,
                      alpha=1,
                      weights=Wtrain,
                      #penalty="SCAD",
                      family = "binomial",
                      type.measure = "class",
                      standardize=T,
                      grouped=F,
                      nfolds=20
                      #nfolds=length(Ytrain)
  )
  
  
  lambda <- fit.cv$lambda.min
  nzero <- fit.cv$nzero[which(fit.cv$lambda == fit.cv$lambda.min)]
  
  if (fit.cv$nzero[which(fit.cv$lambda == fit.cv$lambda.min)] > 0) {
    lambda <- fit.cv$lambda.min
    nzero <- fit.cv$nzero[which(fit.cv$lambda == fit.cv$lambda.min)]
  }
  
  if (nzero < minF) {
    # If no features, select a less-generalizable lambda
    lambda <- fit.cv$lambda[which(fit.cv$nzero >= minF)[1]]
  } 
  
  B <- fit$beta[,which(fit$lambda==lambda)]
  #B <- coef(fit.cv, s=lambda) %>% as.matrix()
  #B <- fit$beta[,which(fit$lambda==fit$lambda[60])]
  #print(B)
  #print(fit.cv$lambda.min)
  #plot(fit.cv)
  if (length(B[B!=0])) {
    #print(c(i, length(B[B!=0])))
    dfX <- data.frame(cbind(Y, X[, B != 0]))
    #lmod<-lm(Y ~ . + 1, data=dfX[-i,])
    #print(lmod)
    #Xtest <- dfX[i,-1]
    #yp <- lmod$coefficients[1] + sum(B*X[i,])# predict on test data
    yp <- predict(fit.cv, newx = X[-i,], s=lambda, type="response")
    assess.glmnet(fit.cv, X[-i,], Y[-i], weights = W[-i], s=fit.cv$lambda.min, family = "binomial")$class # best lambda cv error
    assess.glmnet(fit.cv, X[-i,], Y[-i], weights = W[-i], s=fit.cv$lambda.min, family = "binomial")$auc # best lambda cv auc
    assess.glmnet(fit, X[-i,], Y[-i], weights = W[-i], s=fit.cv$lambda.min, family = "binomial")$class # best lambda cv error = same as first
    
  } else {
    yp <- mean(Ytrain)
  }
  betas[i,] <- B
  Yp[i] <- yp
}
```

Cher's method of nested cross-validation

```{r}
set.seed(0) 
  
nrounds <- 200
nfolds <- 20
#ntest <- 30
n = length(Y)

# X size = 230 x 640
nP <- ncol(X)
nO <- nrow(X) 

# Training Log
Yp.train <- matrix(rep(NA, nO * nO),  ncol = nO) %>% as.data.frame()  # Vector of zeros the size of 176 x 176 

# Prediction Log
nested.train.Yp <- matrix(rep(NA, nO * nrounds),  ncol = nrounds)
nested.train.Ypp <- matrix(rep(NA, nO * nrounds),  ncol = nrounds)
nested.test.Yp <- matrix(rep(NA, nO * nrounds),  ncol = nrounds)
nested.test.Ypp <- matrix(rep(NA, nO * nrounds),  ncol = nrounds)

### Score Log
nested.train.errorscore <- c()
nested.train.aucscore <- c()
nested.test.errorscore <- c()
nested.test.acuscore <- c()

### Coefs Log
Xcoef <- matrix(rep(NA, nP * nrounds),  ncol = nrounds) # Matrix of zeros the dimensions of X (640 x 200)

### Best Lambda Log
nested.lambdas <- c()

#colnames(Xco) <- paste("s", 1:numO, sep="")
#rownames(Xco) <- paste("b", 1:numP, sep="")
for(i in 1:nrounds) {
  tryCatch({ 
  
  # split train/test 
  train = sample(1:n, 3*n/4) # by default replace=FALSE in sample() 
  test = (1:n)[-train]

  iW <- Y[train]
  iW[iW == 0] <- mean(Y[train])
  iW[iW == 1] <- (1-mean(Y[train]))
  
  ilasso <- glmnet(x=X[train, ], y=Y[train], 
                   alpha=1,
                   weights = iW,
                   family = "binomial", 
                   type.measure = "class",  
                   standardize=F)
  
  ilasso.cv <- cv.glmnet(x=X[train, ], y=Y[train], 
                        alpha=1,
                        weights=iW,
                        #penalty="SCAD",
                        family = "binomial",
                        type.measure = "class",
                        standardize=T,
                        grouped=F,
                        nfolds=nfolds)
  
  # define best lambda
  bestlambda <- ilasso.cv$lambda.min
  nested.lambdas <- c(nested.lambdas, bestlambda)
  
  # validation data misclassification error
  ilasso.cv.error <- assess.glmnet(ilasso.cv, X[train,], Y[train], weights = W[train], s=bestlambda, family = "binomial")$class %>% as.vector()# best lambda cv error
  ilasso.cv.auc <- assess.glmnet(ilasso.cv, X[train,], Y[train], weights = W[train], s=bestlambda, family = "binomial")$auc %>% as.vector()# best lambda cv error
  # training data prediction probabilities
  ilasso.cv.pred <- predict(ilasso.cv, newx = X[train,], weights = W[train], s=bestlambda, type="class", family = "binomial")%>% as.numeric()
  ilasso.cv.predprob <- predict(ilasso.cv, newx = X[train,], weights = W[train], s=bestlambda, type="response", family = "binomial")%>% as.numeric()
  
  # testing data misclassification error
  ilasso.test.error <-assess.glmnet(ilasso.cv, X[test,], Y[test], weights = W[test], s=bestlambda, family = "binomial")$class %>% as.vector()# best lambda cv error
  ilasso.test.auc <-assess.glmnet(ilasso.cv, X[test,], Y[test], weights = W[test], s=bestlambda, family = "binomial")$auc %>% as.vector()# best lambda cv error
  # training data prediction probabilities
  ilasso.test.pred <- predict(ilasso.cv, newx = X[test,], weights = W[test], s=bestlambda, type="class", family = "binomial")%>% as.numeric()
  ilasso.test.predprob <- predict(ilasso.cv, newx = X[test,], weights = W[test], s=bestlambda, type="response", family = "binomial")%>% as.numeric()
  
  # coeff
  B <- coef(ilasso.cv, s=bestlambda)[-1,] # do not keep intercept
  
  ### LOG Score
  nested.train.errorscore <- c(nested.train.errorscore, ilasso.cv.error)
  nested.train.aucscore <- c(nested.train.aucscore, ilasso.cv.auc)
  nested.test.errorscore <- c(nested.test.errorscore, ilasso.test.error)
  nested.test.acuscore <- c(nested.test.acuscore, ilasso.test.auc)
  
  ### LOG Coefs
  Xcoef[,i] <- B
  
  ### LOG Prediction
  nested.train.Yp[train,i] <- ilasso.cv.pred
  nested.train.Ypp[train,i] <- ilasso.cv.predprob
  nested.test.Yp[test,i] <- ilasso.test.pred
  nested.test.Ypp[test,i] <- ilasso.test.predprob
  
  # print(paste('running i =', i, 'best lambda:', round(bestlambda,4), "test.error", round(ilasso.test.error,4), "train[1] index", train[1]))

  }, error=function(e){
    print(paste('i=', i, "Uhhh, error\n"))
  })
}
```

### Choose Lambda

We could compare the distribution of nested best vs. fit.cv\$lambda.min

```{r}
gghistogram(nested.lambdas, color = "darkgray",  fill = "darkgray", bins = 50,  
            title = "Nested CV Lambdas vs. Regular CV Lambda", 
            xlab = "Nested CV Lambdas", add = "median", rug = TRUE) + 
  geom_vline(aes(xintercept = fit.cv$lambda.min), col='red') + 
  geom_vline(aes(xintercept = fit.cv$lambda.1se), col='red') + 
  geom_text(aes(label=paste("median\n", round(median(nested.lambdas),4)), y=15, x=mean(nested.lambdas))) + 
  geom_text(aes(label=paste("min lambda\n", round(fit.cv$lambda.min,4)), y=10, x=fit.cv$lambda.min)) +
  geom_text(aes(label=paste("1se lambda\n", round(fit.cv$lambda.1se,4)), y=10, x=fit.cv$lambda.1se)) 
```

The averaged Nested CV lambdas is `r mean(nested.lambdas)`, the median
is `r median(nested.lambdas)`

If we weighted by accuracy score, the weighted mean lambda is
`r weighted.mean(nested.lambdas, 1-nested.test.errorscore)`. Since there
is no big difference between weighted mean and mean, we could simply use
the mean/median directly.

### Refit using nested CV lambda

Next, given the best lambda from nested CV, we could refit the LASSO
model and see the model accuracy

```{r}
nested.lambda <- median(nested.lambdas)

# find optimal lambda using training dataset 

nested.fit = glmnet(X, Y,
             alpha = 1,
             lambda = nested.lambda,
             family = "binomial",
             weights = W,
             type.measure = "class",
             standardize = T,
             keep = T,
             nfolds = length(Y),
             grouped = T)

nested.fit.cv <- cv.glmnet(y = Y,
                      x = X,
                      alpha=1,
                      lambda = c(nested.lambda,
                                 fit.cv$lambda.min),
                      family = "binomial",
                      weights = W,
                      type.measure = "class",
                      standardize=T,
                      nfolds=length(Y),
                      grouped = F, 
                      keep = T)

```

```{r}
plot(fit, sub="Beta Values for Connectivity")  
L1norm <- sum(abs(fit$beta[,which(round(fit$lambda, 3) == round(nested.lambda, 3))]))
abline(v=L1norm, lwd=2, lty=2)
```

```{r}
# training data prediction probabilities
nested.fit.pred <- predict(nested.fit, newx = X, 
                           weights = W, type="class",  family = "binomial")%>% as.numeric()
nested.fit.predprob <- predict(nested.fit,  newx = X,  
                               weights = W, type="response", family = "binomial")%>% as.numeric()
```

### Model Evaluation (Nested CV Refit)

> Accuracy

```{r}

# calculate cross-validation accuracy (LOO)
score <- round(1-as.vector(assess.glmnet(object = nested.fit.cv, 
                                       newx = X, 
                                       newy = Y,
                                       weights = W,
                                       family="binomial")$class), 4) * 100

plot_prediction(Y, nested.fit.pred, score, "(Nested CV Refit)")
```

```{r}
score <- round(as.vector(assess.glmnet(object = nested.fit.cv, 
                                       newx = X, 
                                       newy = Y,
                                       weights = W,
                                       family = "binomial")$auc), 4) 
plot_roc(Y, nested.fit.predprob, score, "(Nested CV Refit)")
```


```{r}
plot_roc_slide(Y, nested.fit.predprob, "(Nested CV Refit)")
```

```{r eval=FALSE, include=FALSE}
df.nested = X %>% as.data.frame() %>% cbind(data.frame(y=Y))
ncv.info <- nested_cv(df.nested,  outside = vfold_cv(repeats = 5),  inside = bootstraps(times = 2))
ncv.info
### The splitting information for each resample is contained in the split objects
# ncv.info$splits[[1]]

### Each element of inner_resamples has its own tibble with the bootstrapping splits
# ncv.info$inner_resamples[[1]]
# ncv.info$inner_resamples[[1]]$splits[[1]]

object <- ncv.info$splits[[1]]

svm_rmse <- function(object, cost = 1) {
  df.dat = analysis(object)
  
  y_col <- ncol(object$data)
  x = df.dat[,1:y_col-1] %>% as.matrix()
  y = df.dat[,y_col] %>% as.matrix()
  mod <- glmnet(x, y, 
             alpha = 1, 
             #lambda = fit.cv$lambda.min,
             family = "binomial",
             #weights = W,
             type.measure = "class",
             standardize=T, 
             grouped = T) ## TODO add weights
  assess.glmnet(mod, newx = x, newy = y)$class
}

```

Visualize Prediction vs. Observed on Training and Testing data ' The Yp
is calculated by averaged across each round

### Model Evaluation (Averaged Performance)

> Accuracy

```{r fig.height=6, fig.width=6} 

plot_prediction(Y, apply(nested.train.Ypp, 1, mean, na.rm=T), -1, "(Averaged Training)")
plot_prediction(Y, apply(nested.test.Ypp, 1, mean, na.rm=T), -1, "(Averaged Testing)")
```

> ROC

```{r fig.height=6, fig.width=6}
plot_roc(Y, apply(nested.train.Ypp, 1, mean, na.rm=T), -1, "(Averaged Training)")
plot_roc(Y, apply(nested.test.Ypp, 1, mean, na.rm=T), -1, "(Averaged Testing)")
```

> ROC By Sliding Threshold

```{r}
plot_roc_slide(Y, apply(nested.train.Ypp, 1, mean, na.rm=T), "(Averaged Training)")
plot_roc_slide(Y, apply(nested.test.Ypp, 1, mean, na.rm=T), "(Averaged Testing)")
```


```{r}
### LOG Score
nested.df <- data.frame(score=1-nested.train.errorscore, data_type="train", score_type="Accuracy", rounds = seq(1:nrounds)) %>%
  rbind(data.frame(score=1-nested.test.errorscore, data_type="test", score_type="Accuracy", rounds = seq(1:nrounds))) %>%
  rbind(data.frame(score=nested.train.aucscore, data_type="train", score_type="AUC", rounds = seq(1:nrounds))) %>%
  rbind(data.frame(score=nested.test.acuscore, data_type="test", score_type="AUC", rounds = seq(1:nrounds))) 
  
ggboxplot(data=nested.df, x="data_type" , y="score", 
          color = "data_type", #fill = "score_type", 
          facet.by = "score_type", add = "jitter") +
  geom_hline(yintercept = 0.5, col = "gray", line_type=1) +
  ggtitle("Nested CV: AUC and Accuracy for both Training and Testing data") +
  ylim(0,1) +
  theme_pander() 
```

The left skewed distribution of Betas is a good sign that betas do not
change significantly across CV Folds

```{r eval=FALSE, include=FALSE}
Xcoef.stats <- data.frame(beta.id=seq(1:dim(Xcoef)[[1]]),beta.mean=apply(Xcoef,1,mean),  beta.sd=apply(Xcoef,1,sd))%>% # 1=Row, 2=Col 
  filter(beta.mean!=0) %>%
  arrange(-beta.sd)

Xcoef.stats %>% 
  gghistogram("beta.sd", bins = 100, fill = "steelblue", color = "white") +
  ggtitle("Distribution of Coefficients SD across Nested CV-folds") +
  theme_pander()
```

### Predictive Connectome

Instead of threshholding betas, we decide to use nested cv to select
best lambda, and refit LASSO model. Thus, the beta were fit by whole
dataset

```{r}
betas <- nested.fit$beta %>% as.vector()
conn_betas <- as_tibble(data.frame(index=I$index, Beta=betas))
connectome <- order %>%
  filter(index %in% I$index) %>%
  inner_join(conn_betas) %>%
  dplyr::select(-censor2) %>%
  filter(Beta != 0) %>%
  
  # reformat connectome
  separate(connection, c("connection1", "connection2"))%>%
  separate(network, sep = "-", c("network1", "network2"), remove = F) %>%
  #filter(str_detect(network, pattern = "-1-")) %>%
  mutate(network1 = ifelse(str_detect(network, pattern = "-1-"), -1, network1)) %>%
  mutate(connection_type = ifelse(network1==network2, "Within", "Between")) %>%
  arrange(index)


# HARD CODE
connectome[connectome$network=="-1-5","network2"] <- "5"
connectome[connectome$network=="-1-7","network2"] <- "7"
connectome[connectome$network=="-1--1","network2"] <- "-1"
connectome[connectome$network=="-1-11","network2"] <- "11"
connectome[connectome$network=="-1-12","network2"] <- "12"
connectome[connectome$network=="-1-13","network2"] <- "13"

connectome_nested  <- connectome
```

```{r}
connectome %>%
  xtable() %>%
  kable(digits = 5) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

```{r eval=FALSE, include=FALSE}
connectome %>% 
  mutate(beta_sign = ifelse(Beta >0, "+", "-")) %>%
  ggdotchart(x = "network_names", y = "Beta",
           color = "beta_sign",                                # Color by groups
           palette = c("steelblue", "tomato"), # Custom color palette
           rotate = TRUE,
           facet.by = "connection_type", 
           sort.by.groups = F,
           sort.val = "desc",          # Sort the value in descending order
           sorting = "descending",                       # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           add.params = list(color = "lightgray", size = 2), # Change segment color and size
           group = "connection_type",                                # Order by groups
           dot.size = 3,                                 # Large dot size
           #label = round(connectome$Beta,2),                        # Add mpg values as dot labels
           #font.label = list(color = "white", size = 9,
           #                  vjust = 0.5),               # Adjust label parameters
           #group = "cyl",
           #y.text.col = TRUE,
           title = paste("Lasso Connection Weights:", dim(connectome)[[1]]),
           ggtheme = theme_pander()) +
  geom_hline(yintercept = 0, linetype = 2, color = "black") 
```

```{r eval=FALSE, include=FALSE}
nested_beta_thresh <- 0.05

uB <- rowMeans(Xcoef)
conn_betas_nested <- as_tibble(data.frame(index=I$index, Beta=uB))
connectome_nested <- order %>%
  filter(index %in% I$index) %>%
  inner_join(conn_betas_nested) %>%
  dplyr::select(-censor2) %>%
  #filter(Beta != 0) %>%
  filter(Beta <= -nested_beta_thresh | Beta >= nested_beta_thresh) %>%

  # reformat connectome
  separate(connection, c("connection1", "connection2"))%>%
  separate(network, sep = "-", c("network1", "network2"), remove = F) %>%
  #filter(str_detect(network, pattern = "-1-")) %>%
  mutate(network1 = ifelse(str_detect(network, pattern = "-1-"), -1, network1)) %>%
  mutate(connection_type = ifelse(network1==network2, "Within", "Between")) %>%
  arrange(index)


# HARD CODE
connectome_nested[connectome_nested$network=="-1-5","network2"] <- "5"
connectome_nested[connectome_nested$network=="-1-7","network2"] <- "7"
connectome_nested[connectome_nested$network=="-1--1","network2"] <- "-1"
connectome_nested[connectome_nested$network=="-1-11","network2"] <- "11"
connectome_nested[connectome_nested$network=="-1-12","network2"] <- "12"
connectome_nested[connectome_nested$network=="-1-13","network2"] <- "13"

```

### Stability of Estimated Beta Weights

And now, let's visualize the beta weights of the connections

```{r}
connectome_nested %>% 
  mutate(beta_sign = ifelse(Beta >0, "+", "-")) %>%
  ggdotchart(x = "network_names", y = "Beta",
           color = "beta_sign",                                # Color by groups
           palette = c("steelblue", "tomato"), # Custom color palette
           rotate = TRUE,
           facet.by = "connection_type", 
           sort.by.groups = F,
           sort.val = "desc",          # Sort the value in descending order
           sorting = "descending",                       # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           add.params = list(color = "lightgray", size = 2), # Change segment color and size
           group = "connection_type",                                # Order by groups
           dot.size = 3,                                 # Large dot size
           #label = round(connectome$Beta,2),                        # Add mpg values as dot labels
           #font.label = list(color = "white", size = 9,
           #                  vjust = 0.5),               # Adjust label parameters
           #group = "cyl",
           #y.text.col = TRUE,
           title = paste("Lasso Connection Weights(Nested):", dim(connectome_nested)[[1]]),
           ggtheme = theme_pander()) +
  geom_hline(yintercept = 0, linetype = 2, color = "black") 
  
```

# Testing the validity of the Lasso model

Here, we will examine the quality of our Lasso model bu doing a series
of tests.

## Ablation test

In the ablation test, we remove all the connections with significant
beta values, and check whether the results are still significant.

```{r}
XX <- X[, conn_betas$Beta == 0]

fit_wo <- glmnet(y = Y,
                 x = XX,
                 alpha=1,
                 lambda = fit$lambda,
                 family = "binomial",
                 type.measure = "class",
                 weights = W,
                 standardize = T
)

fit_wo.cv <- cv.glmnet(y = Y,
                       x = XX,
                       alpha=1,
                       weights = W,
                       lambda = fit$lambda,
                       standardize=T,
                       type.measure = "class",
                       family = "binomial",
                       grouped=F,
                       nfolds=length(Y)
)
```

The model does converge, but its overall classification error is much
higher.

```{r}
plot(fit_wo, sub="Beta Values for Connectivity")

L1norm <- sum(abs(fit_wo$beta[,which(fit_wo$lambda==fit_wo.cv$lambda.min)]))
abline(v=L1norm, lwd=2, lty=2)
```

It is useful to plot the two $\lambda$-curves (with and without the
relevant connections) on the same plot.

```{r fig.width=6, fig.height=4}
lasso_df_wo <- tibble(lambda=fit_wo.cv$lambda, 
                   error=fit_wo.cv$cvm, 
                   sd=fit_wo.cv$cvsd)



lasso_df$Model <- "Full Model"
lasso_df_wo$Model <- "Without the Selected Connections"

lasso_uber <- rbind(lasso_df, lasso_df_wo)

ggplot(lasso_uber, aes(x = lambda, y = error, fill=Model)) +
  #scale_color_d3() +
  #scale_fill_d3()+
  geom_ribbon(aes(ymin = error - sd, 
                  ymax = error + sd), 
              alpha = 0.5,
              #fill="blue"
              ) +
  geom_line(aes(col=Model), lwd=2) +
  xlab(expression(lambda)) +
  ylab("Cross-Validation Error") +
  ggtitle(expression(paste(bold("Cross Validation Error Across "), lambda))) +
  geom_vline(xintercept = fit.cv$lambda.min,
             linetype="dashed") +
  ylim(0,1) +
  theme_pander() +
  theme(legend.position="bottom")
```

## Variance Inflation Factor

Then, we examine the Variance Inflation Factor (VIF). To calculate the
VIF, we need to first create a linear model of the factor effects:

```{r}
dfX <- data.frame(cbind(Y, X[, betas != 0]))
#dfX <- data.frame(cbind(Y, X[conn_betas[conn_betas$Beta!=0,]$index]))
mod<-lm(Y ~ . + 1, as.data.frame(dfX))
```

We can now calculate the VIF and turn the results into a tibble:

```{r}
vifs <- vif(mod)
vifsT <- tibble(VIF = vifs)
```

And, finally, we can plot an histogram of the distribution of VIF
values. VIFs values \< 10 are considered non-collinear; VIFs values \< 5
are great. All of our factors have VIF values that a re *much* smaller
than 5, which implies that they are as close to a normal basis set as
possible.

```{r}
ggplot(vifsT, aes( x =VIF)) +
  geom_histogram(col="white", binwidth = 0.1, fill="blue", alpha=0.4) +
  theme_pander() +
  xlab("VIF Value") +
  ylab("Number of Predictors") +
  ggtitle("Distribution of Variance Inflation Factors")
```

------------------------------------------------------------------------

Save RData

```{r}
save.image(file = "../data/__cache__/worksapce_ml.RData")
```
